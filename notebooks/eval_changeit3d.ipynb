{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamatia/miniconda3/envs/point-e/lib/python3.10/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitting Chamfer 3D\n",
      "Loaded JIT 3D CUDA chamfer distance\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/noamatia/repos/control_point_e\")\n",
    "sys.path.insert(0, \"/home/noamatia/repos/control_point_e/changeit3d\")\n",
    "import tqdm\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from control_point_e import ControlPointE\n",
    "from control_shapenet import ControlShapeNet\n",
    "from point_e.util.plotting import plot_point_cloud\n",
    "from changeit3d.utils.basics import parallel_apply\n",
    "from changeit3d.language.vocabulary import Vocabulary\n",
    "from changeit3d.in_out.pointcloud import pc_loader_from_npz\n",
    "from changeit3d.evaluation.all_metrics import run_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"chair\"\n",
    "num_samples = 500\n",
    "n_sample_points = 2048\n",
    "ckpt = \"epoch=99-step=52000.ckpt\"\n",
    "base_dir = \"/scratch/noam/control_point_e\"\n",
    "top_pc_dir = \"/scratch/noam/shapetalk/point_clouds/scaled_to_align_rendering\"\n",
    "# run_name = \"07_31_2024_17_20_58_utterance_train_chair_val_chair_switch_0_chamfer_0_5\"\n",
    "run_name = \"07_31_2024_20_53_33_utterance_train_chair_val_chair_switch_0_25_chamfer_0_5\"\n",
    "# run_name = \"07_31_2024_20_53_04_utterance_train_chair_val_chair_switch_0_5_chamfer_0_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(base_dir, \"eval_changeit3d\", obj, f\"{num_samples}_random_samples\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\", obj, \"val.csv\")\n",
    "samples_path = os.path.join(output_dir, \"samples.csv\")\n",
    "if not os.path.exists(samples_path):\n",
    "    df = pd.read_csv(dataset_dir)\n",
    "    df = df.sample(num_samples)\n",
    "    df.to_csv(samples_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ControlShapeNet dataset: 100%|██████████| 500/500 [00:01<00:00, 281.25it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = ControlShapeNet(\n",
    "    df=df,\n",
    "    batch_size=6,\n",
    "    device=device,\n",
    "    num_points=1024,\n",
    "    prompt_key=\"utterance\",\n",
    ")\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=6, shuffle=False)\n",
    "model = ControlPointE.load_from_checkpoint(\n",
    "    os.path.join(f\"/scratch/noam/control_point_e/executions\", run_name, \"checkpoints\", ckpt),\n",
    "    lr=7e-5 * 0.4,\n",
    "    dev=device,\n",
    "    batch_size=6,\n",
    "    timesteps=1024,\n",
    "    num_points=1024,\n",
    "    switch_prob=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "output_path = os.path.join(output_dir, f\"{run_name}.pt\")\n",
    "if not os.path.exists(output_path):\n",
    "    for batch in tqdm.tqdm(data_loader):\n",
    "        prompts, source_latents = (batch[\"prompts\"], batch[\"source_latents\"].to(device))\n",
    "        indices = torch.randperm(3072)[:n_sample_points]\n",
    "        curr_output = model.sampler.sample_batch(\n",
    "            guidances=[source_latents, None],\n",
    "            model_kwargs={\"texts\": prompts},\n",
    "            batch_size=6,\n",
    "        )[:, :3, indices]\n",
    "        if output is None:\n",
    "            output = curr_output.detach().cpu()\n",
    "        else:\n",
    "            output = torch.cat((output, curr_output.detach().cpu()), dim=0)\n",
    "    torch.save(output[:num_samples], output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-shape_talk_file', type=str, default=\"/scratch/noam/shapetalk/language/shapetalk_preprocessed_public_version_0.csv\")\n",
    "parser.add_argument('-vocab_file', type=str, default=\"/scratch/noam/shapetalk/language/vocabulary.pkl\")\n",
    "parser.add_argument('-latent_codes_file', type=str, default=\"/scratch/noam/changeit3d/pretrained/shape_latents/pcae_latent_codes.pkl\")    \n",
    "parser.add_argument('-pretrained_changeit3d', type=str, default=\"/scratch/noam/changeit3d/pretrained/changers/pcae_based/all_shapetalk_classes/decoupling_mag_direction/idpen_0.01_sc_False/best_model.pt\")    \n",
    "parser.add_argument('-top_pc_dir', type=str, default=\"/scratch/noam/shapetalk/point_clouds/scaled_to_align_rendering\")\n",
    "parser.add_argument('--restrict_shape_class', type=str, nargs='*', default=['chair'])        \n",
    "parser.add_argument('--pretrained_shape_classifier', type=str,default=\"/scratch/noam/changeit3d/pretrained/pc_classifiers/rs_2022/all_shapetalk_classes/best_model.pkl\")    \n",
    "parser.add_argument('--compute_fpd', type=bool, default=True)\n",
    "parser.add_argument('--shape_part_classifiers_top_dir', type=str, default=\"/scratch/noam/changeit3d/pretrained/part_predictors/shapenet_core_based\")\n",
    "parser.add_argument('--pretrained_oracle_listener', type=str, default=\"/scratch/noam/changeit3d/pretrained/listeners/oracle_listener/all_shapetalk_classes/rs_2023/listener_dgcnn_based/ablation1/best_model.pkl\")\n",
    "parser.add_argument('--shape_generator_type', type=str, default=\"pcae\", choices=[\"pcae\", \"sgf\", \"imnet\"])\n",
    "parser.add_argument('--pretrained_shape_generator', type=str, required=False, default=\"/scratch/noam/changeit3d/pretrained/pc_autoencoders/pointnet/rs_2022/points_4096/all_classes/scaled_to_align_rendering/08-07-2022-22-23-42/best_model.pt\")\n",
    "parser.add_argument('--n_sample_points', type=int, default=2048)\n",
    "parser.add_argument('--sub_sample_dataset', type=int)\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--save_reconstructions', default=False, type=bool)\n",
    "parser.add_argument('--use_timestamp', default=False, type=bool)\n",
    "parser.add_argument('--experiment_tag', type=str)\n",
    "parser.add_argument('--random_seed', type=int, default=2022)\n",
    "parser.add_argument('--log_dir', type=str, default='./logs')\n",
    "parser.add_argument('--clean_train_val_data', type=bool, default=False)\n",
    "parser.add_argument('-pretrained_listener_file', type=str, default=\"/scratch/noam/changeit3d/pretrained/listeners/oracle_listener/all_shapetalk_classes/rs_2023/listener_dgcnn_based/ablation1/best_model.pkl\")\n",
    "parser.add_argument('--batch_size', type=int, default=1024)\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--evaluate_retrieval_version', type=bool, default=False)\n",
    "parser.add_argument('--f', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('stdout_logger')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_shapes = torch.load(output_path).transpose(1, 2).numpy()\n",
    "theta = np.pi * 1 / 2   \n",
    "rotation = np.array(\n",
    "    [\n",
    "        [np.cos(theta), -np.sin(theta), 0.0],\n",
    "        [np.sin(theta), np.cos(theta), 0.0],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "transformed_shapes = transformed_shapes @ rotation\n",
    "theta = np.pi / 2\n",
    "rotation = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, np.cos(theta), -np.sin(theta)],\n",
    "        [0.0, np.sin(theta), np.cos(theta)],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "transformed_shapes = transformed_shapes @ rotation\n",
    "gt_pc_files = df.source_uid.apply(lambda x: osp.join(top_pc_dir, x + \".npz\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_loader =  partial(pc_loader_from_npz, n_samples=n_sample_points, random_seed=2022)\n",
    "gt_pcs = parallel_apply(gt_pc_files, pc_loader, n_processes=20)\n",
    "gt_pcs = np.array(gt_pcs)\n",
    "vocab = Vocabulary.load(args.vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.68 GiB total capacity; 10.46 GiB already allocated; 3.64 GiB free; 18.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mutterance_spelled\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m gt_classes \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msource_object_class\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m----> 3\u001b[0m results_on_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_pcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/point-e/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/evaluation/all_metrics.py:104\u001b[0m, in \u001b[0;36mrun_all_metrics\u001b[0;34m(transformed_shapes, gt_pcs, gt_classes, sentences, vocab, args, logger, network_input_transformations)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mpretrained_oracle_listener:\n\u001b[1;32m    103\u001b[0m     oracle_listener \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39mpretrained_oracle_listener)\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 104\u001b[0m     _, all_boosts, avg_wins, avg_boost \u001b[38;5;241m=\u001b[39m \u001b[43mlistening_fit_on_raw_pcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_pcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moracle_listener\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    107\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAB Average:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_boost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/point-e/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/evaluation/listening_based.py:42\u001b[0m, in \u001b[0;36mlistening_fit_on_raw_pcs\u001b[0;34m(source_pcs, transformed_pcs, tokens_encoded, pretrained_listener, batch_size, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m transformed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(transformed_pcs[batch_idx])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([visual_stimulus, transformed], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained_listener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     44\u001b[0m all_probs\u001b[38;5;241m.\u001b[39mappend(probs)\n",
      "File \u001b[0;32m~/miniconda3/envs/point-e/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/listening_oriented.py:64\u001b[0m, in \u001b[0;36mContextFreeListener.forward\u001b[0;34m(self, tokens, stimuli)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_based_forward(tokens, stimuli)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_based_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstimuli\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/listening_oriented.py:37\u001b[0m, in \u001b[0;36mContextFreeListener.transformer_based_forward\u001b[0;34m(self, tokens, stimuli)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_stimuli):\n\u001b[1;32m     36\u001b[0m     visual_input \u001b[38;5;241m=\u001b[39m stimuli[:, i]            \n\u001b[0;32m---> 37\u001b[0m     visual_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstimulus_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[1;32m     38\u001b[0m     feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([visual_feat, lang_feat], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     logit_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_head(feat)                \n",
      "File \u001b[0;32m~/miniconda3/envs/point-e/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/listening_oriented.py:424\u001b[0m, in \u001b[0;36mDGCNNEncoder.forward\u001b[0;34m(self, xyz)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xyz):\n\u001b[0;32m--> 424\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdgcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/point-e/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/dgcnn.py:101\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[0;34m(self, x, transpose_input_output, spatial_knn, pool)\u001b[0m\n\u001b[1;32m     99\u001b[0m intermediate_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 101\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract_from_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_knn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/dgcnn.py:39\u001b[0m, in \u001b[0;36mget_graph_feature\u001b[0;34m(x, k, idx, subtract)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_points)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     k \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/repos/control_point_e/changeit3d/changeit3d/models/dgcnn.py:18\u001b[0m, in \u001b[0;36mknn\u001b[0;34m(x, k)\u001b[0m\n\u001b[1;32m     16\u001b[0m inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), x)\n\u001b[1;32m     17\u001b[0m xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m pairwise_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mxx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m idx \u001b[38;5;241m=\u001b[39m pairwise_distance\u001b[38;5;241m.\u001b[39mtopk(k\u001b[38;5;241m=\u001b[39mk, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.68 GiB total capacity; 10.46 GiB already allocated; 3.64 GiB free; 18.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "sentences = df.utterance_spelled.values\n",
    "gt_classes = df.source_object_class.values\n",
    "results_on_metrics = run_all_metrics(transformed_shapes, gt_pcs, gt_classes, sentences, vocab, args, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 17/84 [1:17:18<5:04:40, 272.84s/it]\n"
     ]
    }
   ],
   "source": [
    "curr = 0\n",
    "n_examples = 100\n",
    "results_dir = os.path.join(output_dir, \"results\", run_name)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "for batch in tqdm.tqdm(data_loader):\n",
    "    if curr >= n_examples:\n",
    "        break\n",
    "    prompts, source_latents, target_latents = (\n",
    "        batch[\"prompts\"],\n",
    "        batch[\"source_latents\"].to(device),\n",
    "        batch[\"target_latents\"].to(device),\n",
    "    )\n",
    "    samples = model.sampler.sample_batch(\n",
    "        batch_size=6,\n",
    "        model_kwargs={\"texts\": prompts},\n",
    "        guidances=[source_latents, None],\n",
    "    )\n",
    "    pcs = model.sampler.output_to_point_clouds(samples)\n",
    "    for i, (prompt, source_latent, target_latent) in enumerate(zip(prompts, source_latents, target_latents)):\n",
    "        if curr >= n_examples:\n",
    "            break\n",
    "        output_path = os.path.join(results_dir, f\"output_{curr}.png\")\n",
    "        fig = plot_point_cloud(pcs[i], theta=model.theta)\n",
    "        fig.savefig(output_path)\n",
    "        plt.close()\n",
    "        source_path = os.path.join(output_dir, \"results\", \"source\", f\"source_{curr}.png\")\n",
    "        if not os.path.exists(source_path):\n",
    "            samples = model.sampler.sample_batch(\n",
    "                            batch_size=1,\n",
    "                            model_kwargs={},\n",
    "                            prev_samples=source_latent.unsqueeze(0),\n",
    "                        )\n",
    "            pc = model.sampler.output_to_point_clouds(samples)[0]\n",
    "            fig = plot_point_cloud(pc, theta=model.theta)\n",
    "            fig.savefig(source_path)\n",
    "            plt.close()\n",
    "        target_path = os.path.join(output_dir, \"results\", \"target\", f\"target_{curr}.png\")\n",
    "        if not os.path.exists(target_path):\n",
    "            samples = model.sampler.sample_batch(\n",
    "                            batch_size=1,\n",
    "                            model_kwargs={},\n",
    "                            prev_samples=target_latent.unsqueeze(0),\n",
    "                        )       \n",
    "            pc = model.sampler.output_to_point_clouds(samples)[0]\n",
    "            fig = plot_point_cloud(pc, theta=model.theta)\n",
    "            fig.savefig(target_path)\n",
    "            plt.close()\n",
    "        curr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 17/84 [00:00<00:00, 914.44it/s]\n"
     ]
    }
   ],
   "source": [
    "run_names = [\n",
    "    \"07_31_2024_17_20_58_utterance_train_chair_val_chair_switch_0_chamfer_0_5\",\n",
    "    \"07_31_2024_20_53_33_utterance_train_chair_val_chair_switch_0_25_chamfer_0_5\",\n",
    "    \"07_31_2024_20_53_04_utterance_train_chair_val_chair_switch_0_5_chamfer_0_5\",\n",
    "]\n",
    "html = \"\"\"\n",
    "<style>\n",
    "    table {\n",
    "        font-size: 20px;\n",
    "    }\n",
    "</style>\n",
    "<table>\n",
    "\"\"\"\n",
    "html += \"<tr><td>prompt</td><td>source</td><td>target</td>\"\n",
    "for p in [\"0\", \"0_25\", \"0_5\"]:\n",
    "    html += f\"<td>{p}</td>\"\n",
    "html += \"</tr>\\n\"\n",
    "\n",
    "curr = 0\n",
    "n_examples = 100\n",
    "for batch in tqdm.tqdm(data_loader):\n",
    "    if curr >= n_examples:\n",
    "        break\n",
    "    prompts = batch[\"prompts\"]\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        if curr >= n_examples:\n",
    "            break\n",
    "        source_path = os.path.join(\"source\", f\"source_{curr}.png\")\n",
    "        target_path = os.path.join(\"target\", f\"target_{curr}.png\")\n",
    "        html += f\"<tr><td>{prompt}</td><td><img src='{source_path}'></td><td><img src='{target_path}'></td>\"\n",
    "        for run_name in run_names:\n",
    "            output_path = os.path.join(run_name, f\"output_{curr}.png\")\n",
    "            html += f\"<td><img src='{output_path}'></td>\"\n",
    "        html += \"</tr>\\n\"\n",
    "        curr += 1\n",
    "html += \"</table>\"\n",
    "with open(os.path.join(output_dir, \"results\", \"results.html\"), \"w\") as f:\n",
    "    f.write(html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point-e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
