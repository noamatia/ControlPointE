{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the raw ShapeTalk data; e.g., make a vocabulary, spell them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## 1. Each row of final csv contains a single utterance of a given saliency\n",
    "## 2. Tokenize/spell-check these utterances (adding columns: 'utterance_spelled', 'tokens_lens', 'tokens')\n",
    "## 3. Add splits (train/test/val) for each utterance, based on a \"unary\" split concerning shapes (e.g., used for AE)\n",
    "## 4. Create a vocabulary taking care of <UNK> symbol etc. and use it to encode the tokens\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "\n",
    "from changeit3d.language.basics import tokenize_and_spell\n",
    "from changeit3d.language.spelling import token_spelling_dictionary\n",
    "from changeit3d.language.vocabulary import build_vocab\n",
    "from changeit3d.in_out.datasets.shape_talk import expand_df_from_descriptions_to_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.word_tokenize\n",
    "freq_file = '../../data/aux_language/symspell_frequency_dictionary_en_82_765.txt'\n",
    "glove_file = '../../data/aux_language/glove.6B.100d.vocabulary.txt'\n",
    "\n",
    "shape_talk_version = 0\n",
    "shape_talk_file = f'../../data/shapetalk/language/shapetalk/shapetalk_raw_public_version_{shape_talk_version}.csv'\n",
    "\n",
    "random_seed = 2022\n",
    "shape_split_file = f'../../data/shapetalk/misc/unary_split_rs_{random_seed}.csv'\n",
    "\n",
    "verbose = True\n",
    "save_res = False\n",
    "too_short_bound = 0     # if 0 ignore this restriction\n",
    "too_long_utter_prc = 99 # if 0 ignore, else, sentences longer than this percentile will be ignored\n",
    "min_word_freq = 2       # word must exist in training split at least this many times, else mapped to <UNK> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130342\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workerid</th>\n",
       "      <th>utterance_0</th>\n",
       "      <th>utterance_1</th>\n",
       "      <th>utterance_2</th>\n",
       "      <th>utterance_3</th>\n",
       "      <th>utterance_4</th>\n",
       "      <th>assignmentid</th>\n",
       "      <th>worktimeinseconds</th>\n",
       "      <th>source_model_name</th>\n",
       "      <th>source_object_class</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>target_model_name</th>\n",
       "      <th>target_object_class</th>\n",
       "      <th>target_dataset</th>\n",
       "      <th>is_patched</th>\n",
       "      <th>target_uid</th>\n",
       "      <th>source_uid</th>\n",
       "      <th>hard_context</th>\n",
       "      <th>target_original_object_class</th>\n",
       "      <th>source_original_object_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54520</th>\n",
       "      <td>user_556</td>\n",
       "      <td>Is shorter in height.</td>\n",
       "      <td>The table top is not circular.</td>\n",
       "      <td>The table top has a cut at the edge.</td>\n",
       "      <td>Does not have four curved legs at the base.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3V0Z7YWSI0TXEAUON63VUG7C8AIV2V</td>\n",
       "      <td>251.0</td>\n",
       "      <td>7093cec0f1eba67e11f3f1bdf34ac930</td>\n",
       "      <td>table</td>\n",
       "      <td>ShapeNet</td>\n",
       "      <td>8b3bae4e65ee0f67caf7718498824d44</td>\n",
       "      <td>table</td>\n",
       "      <td>ShapeNet</td>\n",
       "      <td>False</td>\n",
       "      <td>table/ShapeNet/8b3bae4e65ee0f67caf7718498824d44</td>\n",
       "      <td>table/ShapeNet/7093cec0f1eba67e11f3f1bdf34ac930</td>\n",
       "      <td>False</td>\n",
       "      <td>table</td>\n",
       "      <td>table</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workerid            utterance_0                     utterance_1  \\\n",
       "54520  user_556  Is shorter in height.  The table top is not circular.   \n",
       "\n",
       "                                utterance_2  \\\n",
       "54520  The table top has a cut at the edge.   \n",
       "\n",
       "                                       utterance_3 utterance_4  \\\n",
       "54520  Does not have four curved legs at the base.         NaN   \n",
       "\n",
       "                         assignmentid  worktimeinseconds  \\\n",
       "54520  3V0Z7YWSI0TXEAUON63VUG7C8AIV2V              251.0   \n",
       "\n",
       "                      source_model_name source_object_class source_dataset  \\\n",
       "54520  7093cec0f1eba67e11f3f1bdf34ac930               table       ShapeNet   \n",
       "\n",
       "                      target_model_name target_object_class target_dataset  \\\n",
       "54520  8b3bae4e65ee0f67caf7718498824d44               table       ShapeNet   \n",
       "\n",
       "       is_patched                                       target_uid  \\\n",
       "54520       False  table/ShapeNet/8b3bae4e65ee0f67caf7718498824d44   \n",
       "\n",
       "                                            source_uid  hard_context  \\\n",
       "54520  table/ShapeNet/7093cec0f1eba67e11f3f1bdf34ac930         False   \n",
       "\n",
       "      target_original_object_class source_original_object_class  \n",
       "54520                        table                        table  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(shape_talk_file)\n",
    "print(len(df))\n",
    "df.sample(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. (expand datadrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique utterances: 536596\n"
     ]
    }
   ],
   "source": [
    "df = expand_df_from_descriptions_to_utterances(df)\n",
    "print('Unique utterances:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. (tokenization, spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymSpell spell-checker loaded: True\n",
      "Loading glove word embeddings.\n",
      "Done. 400000 words loaded.\n",
      "Updating Glove vocabulary with *valid* ShapeTalk words that are missing from it.\n"
     ]
    }
   ],
   "source": [
    "missed_tokens = tokenize_and_spell(df,\n",
    "                                   glove_file=glove_file,\n",
    "                                   freq_file=freq_file,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   token_spelling_dictionary=token_spelling_dictionary)\n",
    "\n",
    "if verbose:\n",
    "    for m in missed_tokens:\n",
    "        print(m, missed_tokens[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 9251\n"
     ]
    }
   ],
   "source": [
    "all_tokens = set()\n",
    "df.tokens.apply(lambda x: all_tokens.update(x));\n",
    "print('Number of tokens', len(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Make-add train/test/val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read unary (AE-based) shape-split and use it to the shapetalk df.\n",
    "split_df = pd.read_csv(shape_split_file)\n",
    "uids_to_split = split_df.groupby('model_uid')['split'].apply(lambda x: list(x)[0]).to_dict()  # dictionary\n",
    "\n",
    "# pass first the info to each source -- target\n",
    "df = df.assign(target_unary_split=df.target_uid.apply(lambda x: uids_to_split[x]))\n",
    "df = df.assign(source_unary_split=df.source_uid.apply(lambda x: uids_to_split[x]))\n",
    "\n",
    "# now, use it to decide the a split for *neural-listening* (i.e., mimic the split for the AE focusing only on target)\n",
    "df = df.assign(listening_split=df.target_unary_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too-long token length threshold at 99-percentile is 16.0.\n"
     ]
    }
   ],
   "source": [
    "## also ignore some corner-cases based on tokens-len\n",
    "if too_short_bound > 0:\n",
    "    ignore_mask = df.tokens_len <= too_short_bound\n",
    "    df.loc[ignore_mask, 'listening_split'] = 'ignore'\n",
    "\n",
    "if too_long_utter_prc > 0:\n",
    "    too_long_len = np.percentile(df[df.listening_split == 'train']['tokens_len'], too_long_utter_prc)\n",
    "    print('Too-long token length threshold at {}-percentile is {}.'.format(too_long_utter_prc, too_long_len))\n",
    "    ignore_mask = df.tokens_len > too_long_len    \n",
    "    df.loc[ignore_mask, 'listening_split'] = 'ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.8432824694928773\n",
      "test 0.09940625722144779\n",
      "val 0.05012150668286756\n",
      "ignore 0.007189766602807326\n"
     ]
    }
   ],
   "source": [
    "for split in df.listening_split.unique():\n",
    "    print(split, (df.listening_split == split).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.8348198644790494\n",
      "test 0.09547965322141798\n",
      "val 0.05010100708913223\n",
      "ignore 0.019599475210400376\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## let's add one final split for testing/training changeIt3DNet systems\n",
    "## \n",
    "\n",
    "\n",
    "## these systems input the **distractor** and attempt to change it  \n",
    "## so our goal here is to use for testing an input geometry that \n",
    "## a) was not seen during training by the underlying shape encoder (e.g, AE) and\n",
    "## b) couple it with language (prompt) that is *not* compatible with it i.e., it was describing the target\n",
    "\n",
    "df = df.assign(changeit_split=df.source_unary_split) # main condition\n",
    "\n",
    "# also use the ignore mask from the listening (\"linguistic\") conditions above\n",
    "df.loc[ignore_mask, 'changeit_split'] = 'ignore'\n",
    "\n",
    "# also, remove utterances that include the word 'distractor(s)' since these could actually refer directly to the distrator and not the target\n",
    "# e.g., \"the distractor has thinner legs\"\n",
    "\n",
    "mask = df.utterance_spelled.apply(lambda x: 'distract' in x)\n",
    "df.loc[mask, 'changeit_split'] = 'ignore'\n",
    "\n",
    "for split in df.changeit_split.unique():\n",
    "    print(split, (df.changeit_split == split).mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4. Make a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using min-word-freq: 2\n",
      "Using a vocabulary with 5883 tokens\n"
     ]
    }
   ],
   "source": [
    "# Now, use the \"train\" listening split to make a vocabulary and encode the tokens\n",
    "train_tokens = df[df.listening_split == 'train']['tokens']\n",
    "print('Using min-word-freq:', min_word_freq)\n",
    "vocab = build_vocab(train_tokens, min_word_freq)\n",
    "print('Using a vocabulary with {} tokens'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/shapetalk/language/shapetalk/vocabulary_new.pkl\n"
     ]
    }
   ],
   "source": [
    "if save_res:\n",
    "    out_vocab_file = osp.join(osp.dirname(shape_talk_file), 'vocabulary.pkl')\n",
    "    print(out_vocab_file)\n",
    "    vocab.save(out_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest-sequence-len: 16\n"
     ]
    }
   ],
   "source": [
    "largest_sequence = df[df.listening_split.isin(['train', 'test', 'val'])]['tokens_len'].max()\n",
    "print('largest-sequence-len:', largest_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vocab to encode tokens as ints (Adds SOS/EOS/PADDING, etc.)\n",
    "df = df.assign(tokens_encoded = df.tokens.apply(lambda x: vocab.encode(x, largest_sequence)))\n",
    "assert all(df.tokens_encoded.apply(lambda x: len(x)) == largest_sequence + 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/shapetalk/language/shapetalk/shapetalk_preprocessed_new_public_version_0.csv\n"
     ]
    }
   ],
   "source": [
    "## Save preprocessed dataframe.\n",
    "out_file = shape_talk_file.replace('shapetalk_raw', 'shapetalk_preprocessed')\n",
    "print(out_file)\n",
    "\n",
    "if save_res:\n",
    "    df.to_csv(out_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "changeit3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "6998593bef615868a01884a31ab15a79588720b64ac6baa396e0d2c68b9e119c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
