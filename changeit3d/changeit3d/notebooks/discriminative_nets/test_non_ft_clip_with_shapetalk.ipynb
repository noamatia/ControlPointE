{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#### For this notebook to you need to further install hugginface's transformers: \n",
    "#### https://huggingface.co/docs/transformers/index\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from changeit3d.in_out.language_contrastive_dataset import LanguageContrastiveDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data = '../../data/shapetalk/language/shapetalk_preprocessed_public_version_0.csv'  # define your paths\n",
    "top_img_dir = '../../data/shapetalk/images/full_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\" \n",
    "batch_size = 256\n",
    "num_workers = 12\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = model_name.replace('/', '_')\n",
    "model = CLIPModel.from_pretrained(model_name).to(device).eval()\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53341\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(st_data)\n",
    "df = df[df.listening_split == 'test']\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"target\"] = df.target_uid.copy()\n",
    "df[\"distractor_1\"] = df.source_uid.copy()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with CLIP's tokenizer ST utterances\n",
    "use_clean_utters = True\n",
    "\n",
    "if use_clean_utters:\n",
    "    utterance_to_use = df.utterance_spelled\n",
    "    utterance_to_use = utterance_to_use.apply(lambda x: x.replace('-er', ''))  # quick way to remove our token for -er/-est adjective -endings\n",
    "    utterance_to_use = utterance_to_use.apply(lambda x: x.replace('-est', '')) \n",
    "else:\n",
    "    utterance_to_use = df.utterance # (the original without spell-checking, etc.)\n",
    "\n",
    "df.tokens_encoded = processor(text=utterance_to_use.tolist(), padding=\"longest\")['input_ids'] # quick & compatible with LanguageContrastiveDataset\n",
    "df = df.drop(columns=['tokens', 'tokens_len']) # we do not use them (drop to avoid confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package in dataset\n",
    "def to_stimulus_func(file_name):    \n",
    "    img = Image.open(osp.join(top_img_dir, file_name + '.png')).convert('RGB')    \n",
    "    return processor(images=img, return_tensors=\"pt\")['pixel_values'][0]\n",
    "\n",
    "dataset = LanguageContrastiveDataset(df, n_distractors=1, to_stimulus_func=to_stimulus_func)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02edbfd1baf49d988e41011c3031dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy tensor(0.5305, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        distractor_img = batch['stimulus'][:,0]\n",
    "        target_img = batch['stimulus'][:,1]\n",
    "        joint_img = torch.cat([distractor_img, target_img]).to(device)\n",
    "        text = batch['tokens'].to(device)\n",
    "        assert all(batch['label'] == 1) # target is last\n",
    "        n_batch = len(text)\n",
    "        res = model(input_ids=text, pixel_values=joint_img)        \n",
    "        distractor_logits = res.logits_per_image[:n_batch,:].diagonal()\n",
    "        target_logits = res.logits_per_image[n_batch:,:].diagonal()\n",
    "        all_logits.append(torch.stack([distractor_logits, target_logits], 0).t().cpu())\n",
    "        \n",
    "        # optional\n",
    "        assert torch.allclose(distractor_logits, res.logits_per_text[:,:n_batch].diagonal())\n",
    "        assert torch.allclose(target_logits, res.logits_per_text[:,n_batch:].diagonal())\n",
    "\n",
    "all_logits = torch.cat(all_logits)\n",
    "guessed_correctly = (all_logits.softmax(1).argmax(1) == 1).double()\n",
    "print('Accuracy', guessed_correctly.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'../../data/pretrained/listeners/all_shapetalk_classes/rs_2022/single_utter/{method_name}_not_finetuned_on_test_data.csv'\n",
    "df['guessed_correctly'] = guessed_correctly.to(bool).tolist()\n",
    "df.to_csv(out_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c3d2",
   "language": "python",
   "name": "c3d2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
